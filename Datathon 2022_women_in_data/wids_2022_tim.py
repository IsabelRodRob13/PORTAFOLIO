# -*- coding: utf-8 -*-
"""WiDS_2022 TIM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AZh9S7vK1W8fW9QSgbLF3dyo7JskzaY-

The WiDS Datathon 2022 focuses on a prediction task involving roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc) as well as the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). Each row in the data corresponds to the a single building observed in a given year. Your task is to predict the Site EUI for each row, given the characteristics of the building and the weather data for the location of the building. 

The Target is site_eui (Site Energy Usage Intensity is the amount of heat and electricity consumed by a building as reflected in utility bills).

https://www.kaggle.com/c/widsdatathon2022/data?select=train.csv

#1 - Import and Install libraries
"""

# Check the versions of libraries

# Python version
import sys
print('Python: {}'.format(sys.version))
# scipy
import scipy
print('scipy: {}'.format(scipy.__version__))
# numpy
import numpy
print('numpy: {}'.format(numpy.__version__))
# matplotlib
import matplotlib
print('matplotlib: {}'.format(matplotlib.__version__))
# pandas
import pandas
print('pandas: {}'.format(pandas.__version__))
# scikit-learn
import sklearn
print('sklearn: {}'.format(sklearn.__version__))

# Commented out IPython magic to ensure Python compatibility.
# Loading Libraries
# data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd

# Generic
import os
import datetime
import itertools
import warnings
warnings.filterwarnings("ignore")

# visualization
import seaborn as sns
from scipy.stats import norm
import matplotlib.pyplot as plt
# %matplotlib inline

# machine learning
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier

## scikit modeling libraries
from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,
                             GradientBoostingClassifier, ExtraTreesClassifier,
                             VotingClassifier)

from sklearn.ensemble import RandomForestRegressor

from sklearn.model_selection import (GridSearchCV, cross_val_score, cross_val_predict,
                                     StratifiedKFold, learning_curve)

## Load metrics for predictive modeling
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_curve, auc

#Preprocessing
import scipy.stats as stats
from sklearn.preprocessing import LabelEncoder
import pickle
import statsmodels.api as sm
from sklearn.impute import KNNImputer
from sklearn.preprocessing import MinMaxScaler

from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Lasso
from sklearn.linear_model import LassoCV
from sklearn.linear_model import ElasticNetCV
from sklearn import linear_model
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error

#PCA libraries
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import scale
import statsmodels.api as sm

#Ensemble methods
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.ensemble import BaggingRegressor, BaggingClassifier
from sklearn.tree import ExtraTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingRegressor

from xgboost import XGBRegressor
from sklearn.svm import SVR
from sklearn import svm
from sklearn.ensemble import ExtraTreesRegressor
from catboost import CatBoostRegressor

#Kaggle conection
!pip install kaggle --upgrade
!kaggle

# This comand dowload the competition
!kaggle competitions download -c widsdatathon2022

!pip install catboost

"""#2 - Loading Dataset"""

path = ("/content/train.csv")
path2 = ("/content/test.csv")

df = pd.read_csv(path,index_col='id')
dft = pd.read_csv(path2,index_col='id')

#dft = pd.read_csv(path2,index_col='id')

"""# 3 - EDA

3.a) Data Overview
"""

df.tail()

df.describe()

df.columns

df.info()

#Determination of shape and size of df
print(df.shape,dft.shape) #(75757,64)
print(df.size,dft.size)  # 4848448

# Correlation between variables
sns.set_theme(style='white')
corr = df.corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
f, ax = plt.subplots(figsize=(20, 15))
cmap = sns.diverging_palette(200, 20, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={'shrink': 0.8});

# Correlation between variables
sns.set_theme(style='white')
corr = dft.corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
f, ax = plt.subplots(figsize=(20, 15))
cmap = sns.diverging_palette(200, 20, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={'shrink': 0.8});

"""3.b) Exploring Nan"""

#Visualization of missing values in DataFrame
fig, ax = plt.subplots(figsize=(16,4))
sns.heatmap(df.isnull(),yticklabels=False,cmap="viridis");

#Visualization of missing values in DataFrame
fig, ax = plt.subplots(figsize=(16,4))
sns.heatmap(dft.isnull(),yticklabels=False,cmap="viridis");

#Identify Nan
def null_rate(x):
  null_count = x.isnull().sum()
  null_percentage = round((x.isnull().sum()/x.shape[0])*100, 2)
  null_df = pd.DataFrame({'column_name' : x.columns,'null_count' : null_count,'null_percentage': null_percentage})
  null_df.reset_index(drop = True, inplace = True)
  null_df = null_df[null_df.null_percentage > 0].sort_values(by = 'null_percentage', ascending = False)
  null_columns = [null_df.column_name]
  return null_df, null_columns

#Cheking nans in DF
exec(f'null_df,null_columns = null_rate(df)')
null_df

#Check Nans in test set
exec(f'null_dft,null_columns = null_rate(dft)')
null_dft

"""Both sets have the same Nans columns

Columns with Nans:


*   energy_star_rating
*   year_built
*   max_wind_speed
*   direction_max_wind_speed
*   direction_peak_wind_speed
*   days_with_fog

3.c) Droping Columns with a significant volum of Nans
"""

#Drop columns with ore than 20% of Nans and input existent values in others
for n,p in zip(null_df.column_name,null_df.null_percentage):
  if p > 40:
    df = df.drop(columns=n)

#Drop columns with ore than 20% of Nans and input existent values in others
for n,p in zip(null_df.column_name,null_df.null_percentage):
  if p > 40:
    dft = dft.drop(columns=n)

"""3.d) Identify Categoricals"""

#Identify categorical
num_cols = df._get_numeric_data().columns
categoricals = df.drop(columns=num_cols)
categoricals

#Identify categorical
num_colst = dft._get_numeric_data().columns
categoricalst = dft.drop(columns=num_colst)
categoricalst

"""3.e) Normalize and KKN imputation of missing values"""

#Nan imputation using KKNImputer
imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')
scaler = MinMaxScaler()

#Normalize and input

#Create only numerical df to normalize
df_num = df.drop(categoricals,axis=1)

#Normalize 
#df_num = pd.DataFrame(scaler.fit_transform(df_num), columns = df_num.columns)

#KNN impute
df_num = pd.DataFrame(imputer.fit_transform(df_num),columns = df_num.columns)

#Concatenate categorical and numericals
df = pd.concat([categoricals,df_num],axis=1)

df

#Normalize and input

#Create only numerical df to normalize
df_numt = dft._get_numeric_data()

#Normalize 
#df_numt = pd.DataFrame(scaler.fit_transform(df_numt), columns = df_numt.columns,index=dft.index)

#KNN impute
df_numt = pd.DataFrame(imputer.fit_transform(df_numt),columns = df_numt.columns,index=dft.index)

#Concatenate categorical and numericals
dft = pd.concat([categoricalst,df_numt],axis=1)

#Checking nan imputation
sns.heatmap(dft.isnull(),yticklabels=False,cmap="viridis");

#Save df after EDA for future modeling
df.to_pickle("/content/Modelar_EDA.pkl")

"""2.f) Outlier detection"""

# Create a DataFrame only with original numercials features
df_out = df.drop(categoricals,axis=1)

#Histogram of numerical features.Cheking the distribution of features (gaussian,normal,...) 
fig, axes = plt.subplots(len(df_out.columns)//3, 3, figsize=(12, 48))

i = 0
for triaxis in axes:
    for axis in triaxis:
        df_out.hist(column = df_out.columns[i], bins = 100, ax=axis)
        i = i+1

df_out.boxplot(column =['cooling_degree_days','heating_degree_days'],rot=60,fontsize=10);

df_out.boxplot(column =[
      'precipitation_inches', 'snowfall_inches',
       'snowdepth_inches', 'avg_temp', 'days_below_30F', 'days_below_20F',
       'days_below_10F', 'days_below_0F', 'days_above_80F', 'days_above_90F',
       'days_above_100F', 'days_above_110F',
       'site_eui'],rot=60,fontsize=10);

df_out.boxplot(column =[
       'january_min_temp', 'january_avg_temp', 'january_max_temp',
       'february_min_temp', 'february_avg_temp', 'february_max_temp',
       'march_min_temp', 'march_avg_temp', 'march_max_temp', 'april_min_temp',
       'april_avg_temp', 'april_max_temp', 'may_min_temp', 'may_avg_temp',
       'may_max_temp', 'june_min_temp', 'june_avg_temp', 'june_max_temp',
       'july_min_temp', 'july_avg_temp', 'july_max_temp', 'august_min_temp',
       'august_avg_temp', 'august_max_temp', 'september_min_temp',
       'september_avg_temp', 'september_max_temp', 'october_min_temp',
       'october_avg_temp', 'october_max_temp', 'november_min_temp',
       'november_avg_temp', 'november_max_temp', 'december_min_temp',
       'december_avg_temp', 'december_max_temp'],rot=60,fontsize=10);

"""2.g) Pair Plots Representation"""

# Pair Plot by month
sns.pairplot(df_out, 
             vars = ['january_min_temp', 'january_max_temp', 'january_avg_temp'], 
             diag_kind = 'kde', 
             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},
             size = 4);# Title 
plt.suptitle('Pair Plot of January´s Temperature', 
             size = 28);

# Pair Plot by month
sns.pairplot(df_out, 
             vars = ['february_min_temp', 'february_max_temp', 'february_avg_temp'], 
             diag_kind = 'kde', 
             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},
             size = 4);# Title 
plt.suptitle('Pair Plot of February´s Temperature', 
             size = 18);

# Pair Plot by energy variables
sns.pairplot(df_out, 
             vars = ['site_eui', 'energy_star_rating'], 
             diag_kind = 'kde', 
             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},
             size = 4);# Title 
plt.suptitle('Pair Plot of Energy', 
             size = 15);

# Pair Plot by below and above temperatures (4 minutes)
sns.pairplot(df_out, 
             vars = ['days_below_30F', 'days_below_20F','days_below_10F','days_below_0F','days_above_80F','days_above_90F','days_above_100F','days_above_110F'], 
             diag_kind = 'kde', 
             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},
             size = 4);# Title 
plt.suptitle('Pair Plot of Energy', 
             size = 15);

print(df.State_Factor.unique())
print(df.building_class.unique())

plt.figure(figsize=(10,5))  
sns.set(style = "darkgrid")
ax = sns.countplot (x ="State_Factor" , data = df, palette = "Set2")
plt.title("State Factor")
plt.xlabel('State type')
plt.ylabel('Quantity')
plt.show()

plt.figure(figsize=(10,5))  
sns.set(style = "darkgrid")
ax = sns.countplot (x ="State_Factor" , data = dft, palette = "Set2")
plt.title("State Factor")
plt.xlabel('State type')
plt.ylabel('Quantity')
plt.show()

print("Clases: ", df['State_Factor'].unique())
for c in df['State_Factor'].unique():
    print(f"The ratio from example {c} is {(sum(df['State_Factor']==c) / df.shape[0]):.2f}")

plt.figure(figsize=(10,5))  
sns.set(style = "darkgrid")
ax = sns.countplot (x ="building_class" , data = df, palette = "Set2")
plt.title("Building class classification")
plt.xlabel('Class of Building ')
plt.ylabel('Quantity by Class')
plt.show()

print("Class: ", df['building_class'].unique())
for c in df['building_class'].unique():
    print(f"The ratio of example {c} is {(sum(df['building_class']==c) / df.shape[0]):.2f}")

df.facility_type.unique()

plt.figure(figsize=(20,10))  #Hay una clase mayoritaria (la 27)
sns.set(style = "darkgrid")
ax = sns.countplot (x ="facility_type" , data = df, palette = "Set2")
plt.title("Installations Class")
plt.xlabel('Facilities type')
plt.ylabel('Quantity by type')
plt.xticks(rotation=90)
plt.show()

"""3.h) Label Encoder Categorical features"""

lbe = LabelEncoder()

#Apply Label Encoder in categorical
categoricals = categoricals.apply(LabelEncoder().fit_transform)

#Concatenate Categorial applied label encode
df = df.drop(columns=categoricals)
df = pd.concat([categoricals, df],axis=1)
df.head(5)

#Apply Label Encoder in categorical
categoricalst = categoricalst.apply(LabelEncoder().fit_transform)

#Concatenate Categorial applied label encode
dft = dft.drop(columns=categoricalst)
dft = pd.concat([categoricalst, dft],axis=1)
dft.head(5)

plt.figure(figsize=(10,5))  
sns.set(style = "darkgrid")
ax = sns.countplot (x ="State_Factor" , data = df, palette = "Set2")
plt.title("Installations Class")
plt.xlabel('State factor')
plt.ylabel('Quantity by Class')
plt.show()

plt.figure(figsize=(10,5))  #Commercial (0) and residential (1)
sns.set(style = "darkgrid")
ax = sns.countplot (x ="building_class" , data = df, palette = "Set2")
plt.title("Installation Class")
plt.xlabel('Building Class')
plt.ylabel('Quantity by Class')
plt.show()

"""Feature correlation"""

def seleccion_parejas(df):
    parejas=[]
    seleccion_numericas=df.select_dtypes(include="number")
    for i in range(len(df.columns)):
        for j in range(0,i+1):
            parejas.append((df.columns[i],df.columns[j]))
    return parejas

seleccion_parejas(df)

def correlaciones(df,n=10):
    df=df.select_dtypes(include="number")
    correlacion=df.corr().abs().unstack()
    correlacion=correlacion.drop(labels=seleccion_parejas(df)).sort_values(ascending=False)
    return correlacion[0:n]

correlaciones(df)

"""3.i) Target variable Analysis"""

df.site_eui.describe()

df.site_eui.mean(axis=0)

df.site_eui.var(axis=0)

"""3.j) Drop Outliers"""

#Eliminating outliers using Z-score
z = np.abs(stats.zscore(df))
dfc = df[(z<3).all(axis=1)]

dfc.boxplot(column =[
      'precipitation_inches', 'snowfall_inches',
       'snowdepth_inches', 'avg_temp', 'days_below_30F', 'days_below_20F',
       'days_below_10F', 'days_below_0F', 'days_above_80F', 'days_above_90F',
       'days_above_100F', 'days_above_110F',
       'site_eui'],rot=60,fontsize=10);

dfc.boxplot(column =['cooling_degree_days','heating_degree_days'],rot=60,fontsize=10);

#Check outliers after apply Z-Score
dfc.boxplot(column =[
       'january_min_temp', 'january_avg_temp', 'january_max_temp',
       'february_min_temp', 'february_avg_temp', 'february_max_temp',
       'march_min_temp', 'march_avg_temp', 'march_max_temp', 'april_min_temp',
       'april_avg_temp', 'april_max_temp', 'may_min_temp', 'may_avg_temp',
       'may_max_temp', 'june_min_temp', 'june_avg_temp', 'june_max_temp',
       'july_min_temp', 'july_avg_temp', 'july_max_temp', 'august_min_temp',
       'august_avg_temp', 'august_max_temp', 'september_min_temp',
       'september_avg_temp', 'september_max_temp', 'october_min_temp',
       'october_avg_temp', 'october_max_temp', 'november_min_temp',
       'november_avg_temp', 'november_max_temp', 'december_min_temp',
       'december_avg_temp', 'december_max_temp'],rot=60,fontsize=10);

"""#4 - PCA"""

# Training model PCA with normalization

pca_pipe = make_pipeline(StandardScaler(), PCA())
pca_pipe.fit(df)

# Train model extraction from pipelne
model_pca = pca_pipe.named_steps['pca']

#loadings values ϕ that will define each component (eigenvector)
model_pca.components_

dft.shape

# Conversion of the array into DataFrame. (64PC)
pd.DataFrame(
    data    = modelo_pca.components_,
    columns = df.columns,
    index   = ['PC0','PC1', 'PC2', 'PC3','PC4', 'PC5', 'PC6','PC7', 'PC8', 'PC9','PC9', 'PC11', 'PC12','PC13', 'PC14', 'PC15','PC16', 'PC17', 'PC18','PC19', 'PC20', 
                'PC21', 'PC22', 'PC23','PC24', 'PC25', 'PC26','PC27', 'PC28', 'PC29','PC30', 'PC31', 'PC32','PC33', 'PC34', 'PC35','P316', 'PC37', 'PC38','PC39', 'PC40',
               'PC41', 'PC42', 'PC43','PC44', 'PC45', 'PC46','PC47', 'PC48','PC49','PC50','PC51', 'PC52', 'PC53','PC54', 'PC55', 'PC56','P557', 'PC58'])

# Heatmap componentes (influence of features in each component)

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))
componentes = model_pca.components_
plt.imshow(componentes.T, cmap='viridis', aspect='auto')
plt.yticks(range(len(df.columns)), df.columns)
plt.xticks(range(len(df.columns)), np.arange(model_pca.n_components_) + 1)
plt.grid(False)
plt.colorbar();

#Variance percentaje explained for each component

print('----------------------------------------------------')
print('Porcentaje de varianza explicada por cada componente')
print('----------------------------------------------------')
print(model_pca.explained_variance_ratio_)

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))
ax.bar(
    x      = np.arange(model_pca.n_components_) + 1,
    height = model_pca.explained_variance_ratio_
)

for x, y in zip(np.arange(len(df.columns)) + 1, model_pca.explained_variance_ratio_):
    label = round(y, 2)
    ax.annotate(
        label,
        (x,y),
        textcoords="offset points",
        xytext=(0,10),
        ha='center'
    )

ax.set_xticks(np.arange(model_pca.n_components_) + 1)
ax.set_ylim(0, 0.4) 
ax.set_title('Porcentaje de varianza explicada por cada componente')
ax.set_xlabel('Componente principal')
ax.set_ylabel('Por. varianza explicada');
#Component 1 explains the 34 % of the observed com variance

#Accumulated variance percentaje explained
# ==============================================================================
prop_varianza_acum = model_pca.explained_variance_ratio_.cumsum()
print('------------------------------------------')
print('Porcentaje de varianza explicada acumulada')
print('------------------------------------------')
print(prop_varianza_acum)

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(30, 20))
ax.plot(
    np.arange(len(df.columns)) + 1,
    prop_varianza_acum,
    marker = 'o'
)

for x, y in zip(np.arange(len(df.columns)) + 1, prop_varianza_acum):
    label = round(y, 2)
    ax.annotate(
        label,
        (x,y),
        textcoords="offset points",
        xytext=(0,10),
        ha='center'
    )
    
ax.set_ylim(0.2, 1.1)
ax.set_xticks(np.arange(model_pca.n_components_) + 1)
ax.set_title('Porcentaje de varianza explicada acumulada')
ax.set_xlabel('Componente principal')
ax.set_ylabel('Por. varianza acumulada');
#Components 1 to 23 explains the 99 % of the observed variance

# Proyección de las observaciones de entrenamiento (reducir la dimensionalidad de nuevas observaciones proyectándolas en el espacio definido por las componentes)

proyecciones = pca_pipe.transform(X=df)
proyecciones = pd.DataFrame(
    proyecciones,
    columns = ['PC0','PC1', 'PC2', 'PC3','PC4', 'PC5', 'PC6','PC7', 'PC8', 'PC9','PC9', 'PC11', 'PC12','PC13', 'PC14', 'PC15','PC16', 'PC17', 'PC18','PC19', 'PC20', 
                'PC21', 'PC22', 'PC23','PC24', 'PC25', 'PC26','PC27', 'PC28', 'PC29','PC30', 'PC31', 'PC32','PC33', 'PC34', 'PC35','P316', 'PC37', 'PC38','PC39', 'PC40',
               'PC41', 'PC42', 'PC43','PC44', 'PC45', 'PC46','PC47', 'PC48','PC49','PC50','PC51', 'PC52', 'PC53','PC54', 'PC55', 'PC56','P557', 'PC58'],
    index   = df.index)
proyecciones.head()

# Recostruccion de las proyecciones (revertir la proyección)

recostruccion = pca_pipe.inverse_transform(proyecciones)
recostruccion = pd.DataFrame(
                    recostruccion,
                    columns = df.columns,
                    index   = df.index
)
print('------------------')
print('Valores reconstruidos')
print('------------------')
display(recostruccion.head())

print('---------------------')
print('Valores originales')
print('---------------------')
display(df.head())

X_pca = recostruccion.drop(['site_eui'], axis= 1)
y_pca = recostruccion['site_eui']

X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(
    X_pca, y_pca, test_size=0.20)

#Apply Random Forest Regressor
clf_pca = RandomForestRegressor()

#fit(X_train_pca,y_train_pca)
clf_pca.fit(X_train_pca,y_train_pca)

predicted_rf_pca = clf_pca.predict(X_test_pca)
predicted_rf_pca

err_pca = mean_squared_error(y_test_pca, predicted_rf_pca, squared=False) 
print("RMSE of validation set: ", err_pca)

"""3.a)PCA with 23 main features"""

# Proyección de 59 a 23 componentes principales
pca_23 = PCA(23)  
projected = pca_23.fit_transform(df)
print(df.shape)
print(projected.shape)

recostruccion_2 = pca_23.inverse_transform(projected)
recostruccion_2 = pd.DataFrame(
                    recostruccion,
                    columns = df.columns,
                    index   = df.index)

X_pca_23 = recostruccion_2.drop(['site_eui'], axis= 1)
y_pca_23 = recostruccion_2['site_eui']

X_train_pca_23, X_test_pca_23, y_train_pca_23, y_test_pca_23 = train_test_split(
    X_pca_23, y_pca_23, test_size=0.20)

#Apply Random Forest Regressor
clf_pca_23 = RandomForestRegressor()

#fit(X_train_pca,y_train_pca)
clf_pca_23.fit(X_train_pca_23,y_train_pca_23)

predicted_rf_pca_23 = clf_pca_23.predict(X_test_pca_23)
predicted_rf_pca_23

err_pca23 = mean_squared_error(y_test_pca_23, predicted_rf_pca_23, squared=False) #RMSE: 0.0355
print("RMSE of validation set: ", err_pca23)

# Evaluating the Algorithm
from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_pca_23, predicted_rf_pca_23))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test_pca_23, predicted_rf_pca_23))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_pca_23, predicted_rf_pca_23)))

"""# 5 - Modeling

Random Forest Regresor
"""

X = df.drop(["site_eui"],axis=1)
y= df["site_eui"]

# Train the mode

forest = RandomForestRegressor(n_estimators = 100)

forest.fit(X,y)

importances = forest.feature_importances_
#
# Sort the feature importance in descending order
#
sorted_indices = np.argsort(importances)[::-1]

plt.figure(figsize=(15,4))
plt.bar(range(X.shape[1]), importances[sorted_indices], align='center',width=1)
plt.xticks(range(X.shape[1]), X.columns[sorted_indices], rotation=90)
plt.show()

#Separation in proportion 80/20 (train/test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30)

#Define models

# Models name
models_name=['Ridge',
             'Lasso',
             'ElasticNet',
             'RandomForestRegressor',
             'GradientBoostingRegressor',
             'XGBRegressor',
             'svm.SVR',
             'ExtraTreesRegressor',
             'LGBMRegressor',
             'BaggingRegressor',
             'AdaBoostRegressor',
             'CatBoostRegressor',
             ]

#Define score functions


# evaluate a given model by making predictions on X_valid
def get_v_score(model):
    valid_predictions=model.predict(X_test)
    score=np.sqrt(mean_squared_error(y_test, valid_predictions))
    return score

# evaluate a given model using cross-validation
def get_cv_score(model, X, y):
    cv = KFold(n_splits=5, shuffle=True, random_state=1)
    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=3, n_jobs=1))
    return np.mean(scores)

# Models
models=[linear_model.Ridge(alpha=.5),
        Lasso(alpha=0.50,tol=0.001),
        ElasticNet(),
        RandomForestRegressor(n_estimators = 100,criterion= 'mse',max_depth= 16,
            max_features = 'auto',oob_score= False,n_jobs = -1,random_state = 123),
        GradientBoostingRegressor(n_estimators = 100,max_depth=16),
        XGBRegressor(n_estimators = 100,max_depth=16),
        svm.SVR(),
        ExtraTreesRegressor(n_estimators=100, n_jobs=4, min_samples_split=25,
                            min_samples_leaf=35),
        LGBMRegressor(),
        BaggingRegressor(n_estimators=100,base_estimator=ExtraTreeRegressor()),
        AdaBoostRegressor(n_estimators=100),
        CatBoostRegressor(iterations=100, depth=16, learning_rate=1, loss_function='RMSE')
        ]

#Fit and get scores for each model
scores_list=[]
for model in models:
    model.fit(X_train,y_train)
    scores_list.append(get_v_score(model))

#Convert list to dataframe
data={'Model':models_name,'RMSE': scores_list}
scores_df=pd.DataFrame(data)
#Sort by valid RMSLE
scores_df.sort_values(by='RMSE').round(5).style.set_properties(**{'background-color': 'black','color': 'white'})

model = XGBRegressor(n_estimators = 100,max_depth=16)
model_fitted = model.fit(X_train,y_train)

dft = dft.drop(columns='site_eui')

dft.head()

dft['site_eui']=model.predict(dft)
submission=dft[['site_eui']].reset_index()
submission.to_csv('WiDS_TIM_submission_19.csv', index=False)

dft

"""LGBMRegressor"""

model_REG = LGBMRegressor()

# train model
model_REG.fit(X_train, y_train)

# predict on validation set
y_pred = model_REG.predict(X_test)

err = mean_squared_error(y_test, y_pred, squared=False)  #Sale RMSE: 0.036
print("RMSE of validation set: ", err)

# Evaluating the Algorithm
from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

"""RandomForest *Regressor*"""

RF_REG = RandomForestRegressor()

RF_REG.fit(X_train, y_train)

y_pred = RF_REG.predict(X_test)

err = mean_squared_error(y_test, y_pred, squared=False)  #Sale RMSE de 0.036
print("RMSE of validation set: ", err)

"""Support Vector Regressor"""

from sklearn.svm import SVR

svr = SVR().fit(X_train, y_train)

y_pred_svr = svr.predict(X_test)

err = mean_squared_error(y_test, y_pred_svr, squared=False) #RSME:0.061
print("RMSE of validation set: ", err)

"""Hyperparameter tuning and cross validation

Métodos de ensamblado
"""

def rmse(y_ver,y_pred):
    return np.sqrt(mean_squared_error(y_ver,y_pred))

def rmse_cv(estimador,X,y):
    pred=estimador.predict(X)
    return rmse(y,pred)

errores={}

estimador_elnet=ElasticNet()
errores["ElasticNet"]=cross_val_score(estimador_elnet,
                                        X=df.drop(['site_eui'],axis=1),
                                        y=df["site_eui"],
                                       scoring=rmse_cv,
                                       cv=10).mean()
estimador_Lasso=Lasso()
errores["Lasso"]=cross_val_score(estimador_Lasso,
                                        X=df.drop(['site_eui'],axis=1),
                                        y=df["site_eui"],
                                       scoring=rmse_cv,
                                       cv=10).mean()
estimador_Ridge=Ridge()
errores["Ridge"]=cross_val_score(estimador_Ridge,
                                        X=df.drop(['site_eui'],axis=1),
                                        y=df["site_eui"],
                                       scoring=rmse_cv,
                                       cv=10).mean()

errores

estimador_bagging_10=BaggingRegressor()
error_cv=cross_val_score(estimador_bagging_10,
                        X=df.drop(['site_eui'],axis=1),
                        y=df["site_eui"],
                        scoring=rmse_cv,cv=10).mean()
errores["Bagging"]=error_cv

errores

estimador_bagging_100=BaggingRegressor(n_estimators=100)  #Tarda 17 minutos
error_cv=cross_val_score(estimador_bagging_100,
                         X=df.drop(['site_eui'],axis=1),
                         y=df["site_eui"],
                        scoring=rmse_cv,cv=10).mean()
errores["Bagging_100"]=error_cv

errores

#Tarda 3 minutos
estimador_bagging_extra_tree=BaggingRegressor(n_estimators=100,
                                        base_estimator=ExtraTreeRegressor())
error_cv=cross_val_score(estimador_bagging_extra_tree,
                        X=df.drop(['site_eui'],axis=1),
                        y=df["site_eui"],
                        scoring=rmse_cv,cv=10).mean()
errores["bagging_extra_tree"]=error_cv

errores

"""Boosting"""

estimador_boosting_Ada=AdaBoostRegressor(n_estimators=100,
                                        )
error_cv=cross_val_score(estimador_boosting_Ada,
                        X=df.drop(['site_eui'],axis=1),
                        y=df["site_eui"],
                        scoring=rmse_cv,cv=10).mean()
errores["boosting_Ada"]=error_cv

errores

#Tarda 2 minuto
estimador_boosting_Ada=AdaBoostRegressor(n_estimators=100,loss='exponential',
                                        )
error_cv=cross_val_score(estimador_boosting_Ada,
                        X=df.drop(['site_eui'],axis=1),
                         y=df["site_eui"],
                        scoring=rmse_cv,cv=10,).mean()
errores["boosting_Ada_Exponential"]=error_cv

errores

"""Gradient Boosting"""

#Tarda un minuto
estimador_gradient_boosting=GradientBoostingRegressor(n_estimators=100,loss='ls',
                                        )
error_cv=cross_val_score(estimador_gradient_boosting,
                        X=df.drop(['site_eui'],axis=1),
                         y=df["site_eui"],
                        scoring=rmse_cv,cv=10,).mean()
errores["gradient_boosting"]=error_cv

errores

"""Prediction on test file"""

X_test_Es =dft

predicted_rf_Es = model_REG.predict(X_test_Es)
predicted_rf_Es

#Guardamos la predicción en formato csv
pd.DataFrame(predicted_rf_Es).to_csv("/content/prediccion.csv",index=True)

#Guardamos el modelo en formato pickle
pickle.dump(model_REG, open('model.pkl', 'wb'))

"""CatBoost Regressor"""

model = CatBoostRegressor(iterations=2,
                          learning_rate=1,
                          depth=2)

model.fit(X_train, y_train,
             eval_set=(X_test,y_test),
             use_best_model=True,
             verbose=True)

print(model.get_all_params())

print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, model.predict(X_test))))

preds = model.predict(X_test_Es)

dft['site_eui'] = preds

df.tail()

#Renaming DataFrame columns to avoid spaces in the column labels.
#df_2.rename(columns={"id": "DF_ID"},inplace=True)

df_2.head()

df['site_eui'] = model.predict(X_test_Es)

submission = dft[['site_eui']].reset_index()

submission

submission.info()

submission.astype({'id': 'int32'}).dtypes

submission.to_csv('TIM_submission.csv', index = False)

#Saving prediction in  csv format
pd.DataFrame(preds).to_csv("/content/submission.csv",index=False)

"""KNNImputer + CatBoost eliminando primero el id y luego volvendo a añadirlo"""

df_best = df_2.drop(['id'], axis=1)

scaler = MinMaxScaler()
df_best = pd.DataFrame(scaler.fit_transform(df_best), columns = df_best.columns)
df_best.head()

#Nan imputation using KKN
imputer_2 = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')

df_best = pd.DataFrame(imputer_2.fit_transform(df_best),columns = df_best.columns)

df_best.tail()

X_test_best =df_best

PRED = model.predict(X_test_best)

df_best['site_eui_2'] = model.predict(X_test_best)

df_best.head()

df_2.rename(columns={"id": "DF_ID"},inplace=True)

data = [df_sub["id"], df_best["site_eui_2"]]
headers = ["id", "site_eui"]
df3 = pd.concat(data, axis=1, keys=headers)

df3.head()

df3.astype({'id': 'int32'}).dtypes

df3['id'] = df3['id'].astype(int)

df3.to_csv('TIM_sub.csv', index = False)

df3.info()